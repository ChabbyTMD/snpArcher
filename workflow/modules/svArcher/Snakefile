import sys
import os
from pathlib import Path

# Get utils. This is not great, but we can move to setup.py and install via pip later if want
utils_path = (Path(workflow.main_snakefile).parent.parent.parent).resolve()
if str(utils_path) not in sys.path:
    sys.path.append(str(utils_path))

import pandas as pd
import snparcher_utils

configfile: "config/config.yaml"
wildcard_constraints:
    window="\d+"

samples = snparcher_utils.parse_sample_sheet(config)
REFGENOME = samples['refGenome'].unique().tolist()

# Define rules here
rule all:
    input:
        expand("results/{refGenome}/SV/{method}/{sample}.vcf.gz", refGenome=REFGENOME, method=["read_pair", "read_depth", "split_read"], sample=samples["BioSample"].unique().tolist())

rule read_pair:
    """
    Rule that implements the read pair methodology using manta
    """
    input:
        unpack(get_bams),
        ref = "results/{refGenome}/data/genome/{refGenome}.fna",
        fai = "results/{refGenome}/data/genome/{refGenome}.fna.fai"
    output:
        rp_vcf = "results/{refGenome}/SV/read_pair/{sample}/{sample}.vcf.gz",
        rp_tbi = "results/{refGenome}/SV/read_pair/{sample}/{sample}.vcf.gz.tbi"
    log:
        "logs/{refGenome}/SV/read_pair/{sample}.txt"
    benchmark:
        "benchmarks/{refGenome}/SV/read_pair/{sample}.txt"
    # resources:
    #     pass
    params:
        outdir = os.path.join(DEFAULT_STORAGE_PREFIX, "results/{refGenome}/SV/read_pair/{sample}/")
    conda:
        "envs/manta.yml"
    shell:
        """
        configManta.py \
            --bam {input.bam} \
            --referenceFasta {input.ref} \
            --runDir {params.outdir}

        python runWorkflow.py -j {threads}

        """

# rule read_depth:
#     """
#     Rule that implements the read depth methodology using ____
#     """
#     input:
#         unpack(get_bams)
#     output:
#         rd_vcf = temp("results/{refGenome}/SV/read_depth/{sample}.vcf.gz"),
#         rd_tbi = temp("results/{refGenome}/SV/read_depth/{sample}.vcf.gz.tbi")
#     log:
#         "logs/{refGenome}/SV/read_depth/{sample}.txt"
#     benchmark:
#         "benchmarks/{refGenome}/SV/read_depth/{sample}.txt"
#     resources:
#         pass
#     conda:
#         "envs/svArcher.yml"
#     shell:
#         pass

rule split_read:
    """
    Rule that implements the split read methodology using lumpy
    """
    input:
        unpack(get_bams)
        fai = "results/{refGenome}/data/genome/{refGenome}.fna.fai"
    output:
        sr_vcf = temp("results/{refGenome}/SV/split_read/{sample}.vcf"),
        sr_gz = "results/{refGenome}/SV/split_read/{sample}.vcf.gz",
        sr_tbi = "results/{refGenome}/SV/split_read/{sample}.vcf.gz.tbi"
    log:
        "logs/{refGenome}/SV/split_read/{sample}.txt"
    benchmark:
        "benchmarks/{refGenome}/SV/split_read/{sample}.txt"
    conda:
        "envs/svArcher.yml"
    shell:
    # Call SVs with lumpy express, compress, reheader and index SV call file
        """
        lumpyexpress \
            -B {input.bam} \
            -o {output.sr_vcf} 2> {log}

        bgzip {output.sr_vcf}

        bcftools reheader -f {input.fai} {output.sr_gz} -o {output.sr_gz} && bcftools sort -Oz -o {output.sr_gz} - 2>>{log}
        
        tabix -p vcf {output.sr_tbi} 2>>{log}
        
        """

# Rule to merge SVS from all methods

# Rule to filter SV calls from all methods

# Rule to combine SV calls per sample

# Rule to benchamrk SV calls against Arabidopsis Gotkay set
