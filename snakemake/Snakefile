import os
import glob
import re
from collections import defaultdict

configfile: "config.yaml"
CLUSTER = json.load(open(config['CLUSTER_JSON']))

# rename variables from config file for clarity downstream
fastq_suffix1 = config["fastq_suffix1"]
fastq_suffix2 = config["fastq_suffix2"]
fastqDir = config["fastqDir"]
newBamDir = config["newBamDir"]
sumstatDir = config["sumstatDir"]

# grab all samples for R1 to get list of names, no need to look at R2 which should have identical names
SAMPLES = glob.glob(fastqDir + "*" + fastq_suffix1)	

print(fastq_suffix1)
print(fastq_suffix2)
print(fastqDir)
print(newBamDir)
print(SAMPLES)

print("SAMPLES:")
for i in range(len(SAMPLES)):
	SAMPLES[i] = os.path.basename(SAMPLES[i])
	SAMPLES[i] = SAMPLES[i].replace(fastq_suffix1, "")
	print(SAMPLES[i])


###
# workflow with rules
###

rule all:
	input:
		#expand(sumstatDir + "{sample}_AlnSumMets.txt", sample=SAMPLES)
		"sumstats.txt"
	
rule bwa_map:
	input:
		ref = config['ref'],
		r1 = fastqDir + "{sample}" + fastq_suffix1,
		r2 = fastqDir + "{sample}" + fastq_suffix2
	output: newBamDir + "{sample}.bam"
	threads: CLUSTER["bwa_map"]["n"]
	params:
		rg="@RG\\tID:{sample}\\tPU:{sample}\\tSM:{sample}\\tPL:{sample}\\tLB:{sample}"
	shell:
		"bwa mem -M -t {threads} -R \'{params.rg}\' {input.ref} {input.r1} {input.r2} | "
		"/n/home11/bjarnold/programs/samtools-1.10/samtools view -Sb - > {output}"

rule sort_bam:
	input: newBamDir + "{sample}.bam"
	output: newBamDir + "{sample}_sorted.bam",
		newBamDir + "{sample}_sorted.bai"
	threads: CLUSTER["sort_bam"]["n"]
	# use run with multiple shell() calls to execute multiple shell commands
	run:
		shell("java -jar /n/home11/bjarnold/picard.jar SortSam TMP_DIR={newBamDir}tmp I={input} O={output[0]} SORT_ORDER=coordinate CREATE_INDEX=true")
		shell("rm {input}")

rule dedup:
	input: newBamDir + "{sample}_sorted.bam",
		newBamDir + "{sample}_sorted.bai",
	output: newBamDir + "{sample}_dedup.bam",
		sumstatDir + "{sample}_dedupMetrics.txt",
		newBamDir + "{sample}_dedup.bai"
	threads: CLUSTER["dedup"]["n"]
	run:
		shell("java -jar /n/home11/bjarnold/picard.jar MarkDuplicates TMP_DIR={newBamDir}tmp I={input[0]} O={output[0]} METRICS_FILE={output[1]} REMOVE_DUPLICATES=false TAGGING_POLICY=All")
		shell("rm {input[0]}")
		shell("mv {input[1]} {output[2]}") # rename index file; marking duplicates should change this

rule bam_sumstats:
	input: newBamDir + "{sample}_dedup.bam",
		ref = config['ref']

	output: sumstatDir + "{sample}.cov",
		sumstatDir + "{sample}_AlnSumMets.txt"
	run:
		shell("/n/home11/bjarnold/programs/samtools-1.10/samtools stats --coverage 0,1000,1 {input[0]} | grep ^COV | cut -f 3- > {output[0]}")
		shell("java -jar /n/home11/bjarnold/picard.jar CollectAlignmentSummaryMetrics I={input[0]} R={input.ref} O={output[1]}")
		
rule collect_sumstats:
	input:
		dedupFiles = expand(sumstatDir + "{sample}_dedupMetrics.txt", sample=SAMPLES),
		alnSumMetsFiles = expand(sumstatDir + "{sample}_AlnSumMets.txt", sample=SAMPLES)	
	output:
		"sumstats.txt"
	run:
		#print(input.dedupFiles)
		#print(input.alnSumMetsFiles)
		PercentDuplicates = collectDedupMetrics(input.dedupFiles)
		PercentHQreads, PercentHQbases = collectAlnSumMets(input.alnSumMetsFiles)
		printStats(PercentDuplicates, PercentHQreads, PercentHQbases)

####
# python helper functions
###

def collectDedupMetrics(dedupFiles):

	PercentDuplicates = defaultdict(float)
	for fn in dedupFiles:
		sample = os.path.basename(fn)
		sample = sample.replace("_dedupMetrics.txt", "")
		f = open(fn, 'r')
		lines = f.readlines()
		for i in range(len(lines)):
			if lines[i].startswith("LIBRARY"):
				info = lines[i+1]
				info = info.split()
				percDup = info[8]
				#print(percDup)
				PercentDuplicates[sample] = percDup
		f.close()
	return(PercentDuplicates)


def collectAlnSumMets(alnSumMetsFiles):

	PercentHQreads = defaultdict(float)
	PercentHQbases = defaultdict(float)

	for fn in alnSumMetsFiles:
		sample = os.path.basename(fn)
		sample = sample.replace("_AlnSumMets.txt", "")
		f = open(fn, 'r')
		for line in f:
			if line.startswith("PAIR"):
				line = line.split()
				totalReads = int(line[1])
				totalAlignedBases = int(line[7])
				HQreads = int(line[8])
				totalAlignedHQ20bases = int(line[10])
				PercentHQreads[sample] = HQreads/totalReads
				PercentHQbases[sample] = totalAlignedHQ20bases/totalAlignedBases
		f.close()
	return(PercentHQreads, PercentHQbases)


def printStats(PercentDuplicates, PercentHQreads, PercentHQbases):

	o = open("sumstats.txt", 'w')
	print("sample", "PercentDuplicates", "PercentHQalignedReads", "PercentHQ20bases", file=o)
	for sample in PercentDuplicates:
		print(sample, PercentDuplicates[sample], PercentHQreads[sample], PercentHQbases[sample], file=o)


